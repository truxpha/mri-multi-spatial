{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import mat73\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from scipy.signal import find_peaks\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from sklearn.metrics import r2_score\n",
    "import psutil\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtr-phan\u001b[0m (\u001b[33mtrphan\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\Proton Drive\\My files\\Schule\\Master\\Masterarbeit\\project\\models\\spatial_signal_full_patch\\wandb\\run-20250213_233443-kj199fk7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/trphan/master-multicomponent-mri/runs/kj199fk7' target=\"_blank\">unet-baseline-fullpatch-improved-3</a></strong> to <a href='https://wandb.ai/trphan/master-multicomponent-mri' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/trphan/master-multicomponent-mri' target=\"_blank\">https://wandb.ai/trphan/master-multicomponent-mri</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/trphan/master-multicomponent-mri/runs/kj199fk7' target=\"_blank\">https://wandb.ai/trphan/master-multicomponent-mri/runs/kj199fk7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.login()\n",
    "# run = wandb.init(project=\"master-multicomponent-mri\", name=\"unet-baseline-fullpatch\")\n",
    "run = wandb.init(project=\"master-multicomponent-mri\", name=\"unet-baseline-fullpatch-att-3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of input data: (9, 9, 8, 47932)\n",
      "Shape of reference data: (9, 9, 32, 47932)\n"
     ]
    }
   ],
   "source": [
    "data = mat73.loadmat('../../data/training_data_T1_3D_9x9x32x47932_noise0.05.mat')\n",
    "input_noisy_np = data['input_noisy']\n",
    "input_clean_np = data['input']\n",
    "ref_np = data['ref']\n",
    "\n",
    "print(\"Shape of input data:\", input_noisy_np.shape)\n",
    "print(\"Shape of reference data:\", ref_np.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN count in input_noisy before handling: 1635840\n",
      "NaN count in ref before handling: 0\n"
     ]
    }
   ],
   "source": [
    "nan_count_input_noisy_before = np.isnan(input_noisy_np).sum()\n",
    "nan_count_ref_before = np.isnan(ref_np).sum()\n",
    "print(f\"NaN count in input_noisy before handling: {nan_count_input_noisy_before}\")\n",
    "print(f\"NaN count in ref before handling: {nan_count_ref_before}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN count in input_noisy after handling: 0\n"
     ]
    }
   ],
   "source": [
    "input_noisy_np = np.nan_to_num(input_noisy_np, nan=0.0)\n",
    "nan_count_input_noisy_after = np.isnan(input_noisy_np).sum()\n",
    "print(f\"NaN count in input_noisy after handling: {nan_count_input_noisy_after}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class T1PatchDataset(Dataset):\n",
    "    def __init__(self, input_data, target_data):\n",
    "        self.input_data = input_data\n",
    "        self.target_data = target_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_data[idx], self.target_data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, in_channels//8, kernel_size=1),\n",
    "            nn.BatchNorm2d(in_channels//8),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Conv2d(in_channels//8, in_channels, kernel_size=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        attention_weights = self.attention(x)\n",
    "        return x * attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, in_channels, reduction_ratio=16):\n",
    "        super().__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(in_channels, in_channels // reduction_ratio),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(in_channels // reduction_ratio, in_channels)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        \n",
    "        avg_out = self.fc(self.avg_pool(x).view(b, c))\n",
    "        max_out = self.fc(self.max_pool(x).view(b, c))\n",
    "        \n",
    "        out = avg_out + max_out\n",
    "        return torch.sigmoid(out).view(b, c, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DualAttentionBlock(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "        self.channel_att = ChannelAttention(in_channels)\n",
    "        self.spatial_att = SpatialAttention(in_channels)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x * self.channel_att(x)\n",
    "        x = x * self.spatial_att(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RefineBlock(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "        self.refine = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(in_channels),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            DualAttentionBlock(in_channels)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x + self.refine(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualDoubleConvWithAttention(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Dropout2d(0.1),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU(0.1)\n",
    "        )\n",
    "        self.attention = SpatialAttention(out_channels)\n",
    "        \n",
    "        # residual connection\n",
    "        self.residual = nn.Identity() if in_channels == out_channels else \\\n",
    "                       nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = self.residual(x)\n",
    "        x = self.double_conv(x)\n",
    "        x = self.attention(x)\n",
    "        return x + identity  # Residual connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedUNet(nn.Module):\n",
    "    def __init__(self, in_channels=8, init_features=64):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder1 = ResidualDoubleConvWithAttention(in_channels, init_features)\n",
    "        self.pool1 = nn.MaxPool2d(2, padding=1)\n",
    "        self.encoder2 = ResidualDoubleConvWithAttention(init_features, init_features*2)\n",
    "        \n",
    "        # Bridge\n",
    "        self.bridge = ResidualDoubleConvWithAttention(init_features*2, init_features*4)\n",
    "        \n",
    "        # Decoder\n",
    "        self.upconv1 = nn.ConvTranspose2d(init_features*4, init_features*2, kernel_size=2, stride=2)\n",
    "        self.decoder1 = ResidualDoubleConvWithAttention(init_features*3, init_features*2)\n",
    "        \n",
    "        # Skip connection refinement\n",
    "        self.refine1 = RefineBlock(init_features)\n",
    "        \n",
    "        # Multi-scale feature fusion\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Conv2d(init_features*2 + init_features, init_features, kernel_size=1),\n",
    "            nn.BatchNorm2d(init_features),\n",
    "            nn.LeakyReLU(0.1)\n",
    "        )\n",
    "        \n",
    "        # Final output\n",
    "        self.final = nn.Sequential(\n",
    "            nn.Conv2d(init_features, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            DualAttentionBlock(64),\n",
    "            nn.Conv2d(64, 32, kernel_size=1)\n",
    "        )\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):\n",
    "            nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.BatchNorm2d):\n",
    "            nn.init.constant_(m.weight, 1)\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoding\n",
    "        enc1 = self.encoder1(x)\n",
    "        x = self.pool1(enc1)\n",
    "        enc2 = self.encoder2(x)\n",
    "        \n",
    "        # Bridge\n",
    "        x = self.bridge(enc2)\n",
    "        \n",
    "        # Decoding\n",
    "        x = self.upconv1(x)\n",
    "        x = x[:, :, :9, :9]\n",
    "        \n",
    "        # skip connection\n",
    "        refined_skip = self.refine1(enc1)\n",
    "        x = torch.cat([x, refined_skip], dim=1)\n",
    "        x = self.decoder1(x)\n",
    "        \n",
    "        # Multi-scale feature fusion\n",
    "        x = self.fusion(torch.cat([x, refined_skip], dim=1))\n",
    "        \n",
    "        # Final output\n",
    "        x = self.final(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepSupervisionLoss(nn.Module):\n",
    "    def __init__(self, main_loss_weight=1.0, aux_loss_weight=0.4):\n",
    "        super().__init__()\n",
    "        self.main_loss_weight = main_loss_weight\n",
    "        self.aux_loss_weight = aux_loss_weight\n",
    "        self.criterion = nn.MSELoss()\n",
    "        \n",
    "    def forward(self, outputs, targets):\n",
    "        if isinstance(outputs, tuple):\n",
    "            main_out, deep_out1, deep_out2 = outputs\n",
    "            loss = self.main_loss_weight * self.criterion(main_out, targets)\n",
    "            loss += self.aux_loss_weight * self.criterion(deep_out1, targets)\n",
    "            loss += self.aux_loss_weight * self.criterion(deep_out2, targets)\n",
    "            return loss\n",
    "        return self.criterion(outputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=15, min_delta=1e-6, path='saved_model/best_unet_model_attention_3.pt'):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = float('inf')\n",
    "        self.early_stop = False\n",
    "        self.path = path\n",
    "        \n",
    "    def __call__(self, val_loss, model):\n",
    "        if val_loss < self.best_loss:\n",
    "            print(f'Validation loss decreased ({self.best_loss:.6f} --> {val_loss:.6f}). Saving model...')\n",
    "            self.best_loss = val_loss\n",
    "            self.save_checkpoint(model)\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "    def save_checkpoint(self, model):\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'val_loss': self.best_loss,\n",
    "        }, self.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transpose the input and reference data to be (N, C, H, W) format\n",
    "input_noisy_torch = np.transpose(input_noisy_np, (3, 2, 0, 1))\n",
    "ref_torch = np.transpose(ref_np, (3, 2, 0, 1))\n",
    "\n",
    "input_noisy_torch = torch.tensor(input_noisy_torch, dtype=torch.float32)\n",
    "ref_torch = torch.tensor(ref_torch, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_mean = input_noisy_torch.mean()\n",
    "input_std = input_noisy_torch.std()\n",
    "input_noisy_torch = (input_noisy_torch - input_mean) / input_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataset\n",
    "dataset = T1PatchDataset(input_noisy_torch, ref_torch)\n",
    "\n",
    "# Split dataset\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ImprovedUNet().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n",
    "early_stopping = EarlyStopping(patience=10, path='best_unet_model_attention_3.pt')\n",
    "\n",
    "num_epochs = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log configuration\n",
    "wandb.config.update({\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"epochs\": num_epochs,\n",
    "    \"architecture\": \"UNet\",\n",
    "    \"optimizer\": \"Adam\",\n",
    "    \"loss_function\": \"MSELoss\",\n",
    "    \"scheduler\": \"ReduceLROnPlateau\",\n",
    "    \"early_stopping_patience\": 10\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/200] Training: 100%|██████████| 600/600 [00:14<00:00, 42.51it/s, loss=0.00418]\n",
      "Epoch [1/200] Validation: 100%|██████████| 150/150 [00:01<00:00, 123.05it/s, loss=0.00322]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (inf --> 0.003122). Saving model...\n",
      "Epoch [1/200], Train Loss: 0.004804139414336533, Val Loss: 0.0031223810153702893, LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [2/200] Training: 100%|██████████| 600/600 [00:14<00:00, 42.76it/s, loss=0.00347]\n",
      "Epoch [2/200] Validation: 100%|██████████| 150/150 [00:01<00:00, 120.52it/s, loss=0.00285]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (0.003122 --> 0.002640). Saving model...\n",
      "Epoch [2/200], Train Loss: 0.00294046962284483, Val Loss: 0.0026400376080224913, LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [3/200] Training: 100%|██████████| 600/600 [00:14<00:00, 41.77it/s, loss=0.00273]\n",
      "Epoch [3/200] Validation: 100%|██████████| 150/150 [00:01<00:00, 123.63it/s, loss=0.00271]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (0.002640 --> 0.002531). Saving model...\n",
      "Epoch [3/200], Train Loss: 0.0025970397365745156, Val Loss: 0.0025309902740021546, LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [4/200] Training: 100%|██████████| 600/600 [00:14<00:00, 42.38it/s, loss=0.00217]\n",
      "Epoch [4/200] Validation: 100%|██████████| 150/150 [00:01<00:00, 120.17it/s, loss=0.00252]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (0.002531 --> 0.002326). Saving model...\n",
      "Epoch [4/200], Train Loss: 0.002401989675903072, Val Loss: 0.0023259759088978173, LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [5/200] Training: 100%|██████████| 600/600 [00:14<00:00, 42.79it/s, loss=0.00214]\n",
      "Epoch [5/200] Validation: 100%|██████████| 150/150 [00:01<00:00, 113.15it/s, loss=0.00249]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch [5/200], Train Loss: 0.0022823925242604066, Val Loss: 0.0023279787716455756, LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [6/200] Training: 100%|██████████| 600/600 [00:14<00:00, 41.14it/s, loss=0.00273]\n",
      "Epoch [6/200] Validation: 100%|██████████| 150/150 [00:01<00:00, 121.52it/s, loss=0.00217]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (0.002326 --> 0.002058). Saving model...\n",
      "Epoch [6/200], Train Loss: 0.0021944624942261725, Val Loss: 0.002057666053685049, LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [7/200] Training: 100%|██████████| 600/600 [00:13<00:00, 43.52it/s, loss=0.00184]\n",
      "Epoch [7/200] Validation: 100%|██████████| 150/150 [00:01<00:00, 118.43it/s, loss=0.00264]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch [7/200], Train Loss: 0.0021302095123489078, Val Loss: 0.0023583986586891113, LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [8/200] Training: 100%|██████████| 600/600 [00:14<00:00, 42.74it/s, loss=0.00272]\n",
      "Epoch [8/200] Validation: 100%|██████████| 150/150 [00:01<00:00, 111.34it/s, loss=0.00224]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 2 out of 10\n",
      "Epoch [8/200], Train Loss: 0.002081057935526284, Val Loss: 0.002183212290207545, LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [9/200] Training: 100%|██████████| 600/600 [00:14<00:00, 41.84it/s, loss=0.00193]\n",
      "Epoch [9/200] Validation: 100%|██████████| 150/150 [00:01<00:00, 103.73it/s, loss=0.00213]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (0.002058 --> 0.002009). Saving model...\n",
      "Epoch [9/200], Train Loss: 0.0020363354766353343, Val Loss: 0.0020092405860001844, LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [10/200] Training: 100%|██████████| 600/600 [00:14<00:00, 41.98it/s, loss=0.00307]\n",
      "Epoch [10/200] Validation: 100%|██████████| 150/150 [00:01<00:00, 103.70it/s, loss=0.00207]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (0.002009 --> 0.001998). Saving model...\n",
      "Epoch [10/200], Train Loss: 0.0020041667765084035, Val Loss: 0.001997783982660621, LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [11/200] Training: 100%|██████████| 600/600 [00:14<00:00, 41.34it/s, loss=0.00302]\n",
      "Epoch [11/200] Validation: 100%|██████████| 150/150 [00:01<00:00, 107.41it/s, loss=0.00201]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (0.001998 --> 0.001934). Saving model...\n",
      "Epoch [11/200], Train Loss: 0.0019749013881664723, Val Loss: 0.0019341644023855528, LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [12/200] Training: 100%|██████████| 600/600 [00:14<00:00, 42.59it/s, loss=0.00283]\n",
      "Epoch [12/200] Validation: 100%|██████████| 150/150 [00:01<00:00, 108.05it/s, loss=0.00205]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (0.001934 --> 0.001911). Saving model...\n",
      "Epoch [12/200], Train Loss: 0.0019378996785962953, Val Loss: 0.001911228314662973, LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [13/200] Training: 100%|██████████| 600/600 [00:14<00:00, 42.51it/s, loss=0.00192]\n",
      "Epoch [13/200] Validation: 100%|██████████| 150/150 [00:01<00:00, 112.06it/s, loss=0.00232]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch [13/200], Train Loss: 0.001917302857618779, Val Loss: 0.002080636476166546, LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [14/200] Training: 100%|██████████| 600/600 [00:14<00:00, 41.92it/s, loss=0.00253]\n",
      "Epoch [14/200] Validation: 100%|██████████| 150/150 [00:01<00:00, 113.47it/s, loss=0.00196]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (0.001911 --> 0.001840). Saving model...\n",
      "Epoch [14/200], Train Loss: 0.0018989571026759222, Val Loss: 0.0018397855029130975, LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [15/200] Training: 100%|██████████| 600/600 [00:13<00:00, 45.44it/s, loss=0.00181]\n",
      "Epoch [15/200] Validation: 100%|██████████| 150/150 [00:01<00:00, 120.77it/s, loss=0.00186]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch [15/200], Train Loss: 0.0018719614710425959, Val Loss: 0.0018512083396005133, LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [16/200] Training: 100%|██████████| 600/600 [00:14<00:00, 40.34it/s, loss=0.00262]\n",
      "Epoch [16/200] Validation: 100%|██████████| 150/150 [00:01<00:00, 116.75it/s, loss=0.00215]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 2 out of 10\n",
      "Epoch [16/200], Train Loss: 0.0018543166694386553, Val Loss: 0.0019167674736430248, LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [17/200] Training: 100%|██████████| 600/600 [00:14<00:00, 42.10it/s, loss=0.00256]\n",
      "Epoch [17/200] Validation: 100%|██████████| 150/150 [00:01<00:00, 109.80it/s, loss=0.00198]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 3 out of 10\n",
      "Epoch [17/200], Train Loss: 0.001846499239715437, Val Loss: 0.0019236950553022324, LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [18/200] Training: 100%|██████████| 600/600 [00:13<00:00, 43.15it/s, loss=0.0028] \n",
      "Epoch [18/200] Validation: 100%|██████████| 150/150 [00:01<00:00, 116.22it/s, loss=0.00194]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (0.001840 --> 0.001791). Saving model...\n",
      "Epoch [18/200], Train Loss: 0.0018258441926445811, Val Loss: 0.0017913085144634047, LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [19/200] Training: 100%|██████████| 600/600 [00:14<00:00, 40.22it/s, loss=0.00189]\n",
      "Epoch [19/200] Validation: 100%|██████████| 150/150 [00:01<00:00, 120.60it/s, loss=0.00213]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch [19/200], Train Loss: 0.0018086022509184356, Val Loss: 0.0020406962395645677, LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [20/200] Training: 100%|██████████| 600/600 [00:13<00:00, 43.18it/s, loss=0.00238]\n",
      "Epoch [20/200] Validation: 100%|██████████| 150/150 [00:01<00:00, 114.63it/s, loss=0.00186]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 2 out of 10\n",
      "Epoch [20/200], Train Loss: 0.001797759125280815, Val Loss: 0.0019171474982673923, LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [21/200] Training: 100%|██████████| 600/600 [00:13<00:00, 42.95it/s, loss=0.0014] \n",
      "Epoch [21/200] Validation: 100%|██████████| 150/150 [00:01<00:00, 126.30it/s, loss=0.00196]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 3 out of 10\n",
      "Epoch [21/200], Train Loss: 0.0017838441394269467, Val Loss: 0.0018243598286062479, LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [22/200] Training: 100%|██████████| 600/600 [00:14<00:00, 41.72it/s, loss=0.0022] \n",
      "Epoch [22/200] Validation: 100%|██████████| 150/150 [00:01<00:00, 115.62it/s, loss=0.00185]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (0.001791 --> 0.001765). Saving model...\n",
      "Epoch [22/200], Train Loss: 0.0017668734496692196, Val Loss: 0.0017649862356483937, LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [23/200] Training: 100%|██████████| 600/600 [00:14<00:00, 42.35it/s, loss=0.00189]\n",
      "Epoch [23/200] Validation: 100%|██████████| 150/150 [00:01<00:00, 125.72it/s, loss=0.00193]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch [23/200], Train Loss: 0.0017518920798708375, Val Loss: 0.0017675568621295195, LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [24/200] Training: 100%|██████████| 600/600 [00:15<00:00, 39.57it/s, loss=0.00141]\n",
      "Epoch [24/200] Validation: 100%|██████████| 150/150 [00:01<00:00, 112.09it/s, loss=0.0019] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (0.001765 --> 0.001749). Saving model...\n",
      "Epoch [24/200], Train Loss: 0.00174029302453467, Val Loss: 0.001749473560291032, LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [25/200] Training: 100%|██████████| 600/600 [00:13<00:00, 44.87it/s, loss=0.00352]\n",
      "Epoch [25/200] Validation: 100%|██████████| 150/150 [00:01<00:00, 125.98it/s, loss=0.00181]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch [25/200], Train Loss: 0.0017300998142066723, Val Loss: 0.0018174676783382893, LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [26/200] Training: 100%|██████████| 600/600 [00:13<00:00, 43.31it/s, loss=0.00188]\n",
      "Epoch [26/200] Validation: 100%|██████████| 150/150 [00:01<00:00, 121.00it/s, loss=0.00187]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (0.001749 --> 0.001745). Saving model...\n",
      "Epoch [26/200], Train Loss: 0.0017192947713192553, Val Loss: 0.0017446812358684837, LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [27/200] Training: 100%|██████████| 600/600 [00:13<00:00, 45.41it/s, loss=0.00213]\n",
      "Epoch [27/200] Validation: 100%|██████████| 150/150 [00:01<00:00, 120.38it/s, loss=0.00193]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch [27/200], Train Loss: 0.0017047171634233867, Val Loss: 0.00180997480560715, LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [28/200] Training: 100%|██████████| 600/600 [00:12<00:00, 46.64it/s, loss=0.00187]\n",
      "Epoch [28/200] Validation: 100%|██████████| 150/150 [00:01<00:00, 121.44it/s, loss=0.00199]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 2 out of 10\n",
      "Epoch [28/200], Train Loss: 0.0016939710090324903, Val Loss: 0.0018254950370950004, LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [29/200] Training: 100%|██████████| 600/600 [00:14<00:00, 42.55it/s, loss=0.00191]\n",
      "Epoch [29/200] Validation: 100%|██████████| 150/150 [00:01<00:00, 125.97it/s, loss=0.0019] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 3 out of 10\n",
      "Epoch [29/200], Train Loss: 0.0016798039601417258, Val Loss: 0.0018050136153275767, LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [30/200] Training: 100%|██████████| 600/600 [00:13<00:00, 45.64it/s, loss=0.002]  \n",
      "Epoch [30/200] Validation: 100%|██████████| 150/150 [00:01<00:00, 136.35it/s, loss=0.00192]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 4 out of 10\n",
      "Epoch [30/200], Train Loss: 0.0016703810989080617, Val Loss: 0.0017873733886517584, LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [31/200] Training: 100%|██████████| 600/600 [00:13<00:00, 43.62it/s, loss=0.00266]\n",
      "Epoch [31/200] Validation: 100%|██████████| 150/150 [00:01<00:00, 128.44it/s, loss=0.00199]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 5 out of 10\n",
      "Epoch [31/200], Train Loss: 0.0016573349372871842, Val Loss: 0.0018578331917524337, LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [32/200] Training: 100%|██████████| 600/600 [00:13<00:00, 44.09it/s, loss=0.00249]\n",
      "Epoch [32/200] Validation: 100%|██████████| 150/150 [00:01<00:00, 128.08it/s, loss=0.00203]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 6 out of 10\n",
      "Epoch [32/200], Train Loss: 0.001645856317675983, Val Loss: 0.001827547150508811, LR: 0.0005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [33/200] Training: 100%|██████████| 600/600 [00:13<00:00, 43.63it/s, loss=0.00201]\n",
      "Epoch [33/200] Validation: 100%|██████████| 150/150 [00:01<00:00, 121.01it/s, loss=0.00182]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 7 out of 10\n",
      "Epoch [33/200], Train Loss: 0.0015688439221897472, Val Loss: 0.001783499448404958, LR: 0.0005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [34/200] Training: 100%|██████████| 600/600 [00:13<00:00, 44.69it/s, loss=0.00214]\n",
      "Epoch [34/200] Validation: 100%|██████████| 150/150 [00:01<00:00, 111.92it/s, loss=0.00181]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 8 out of 10\n",
      "Epoch [34/200], Train Loss: 0.0015493095594380673, Val Loss: 0.0017704903497360648, LR: 0.0005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [35/200] Training: 100%|██████████| 600/600 [00:12<00:00, 47.21it/s, loss=0.00135]\n",
      "Epoch [35/200] Validation: 100%|██████████| 150/150 [00:01<00:00, 123.90it/s, loss=0.00178]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 9 out of 10\n",
      "Epoch [35/200], Train Loss: 0.0015293648516914496, Val Loss: 0.0017522995662875474, LR: 0.0005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [36/200] Training: 100%|██████████| 600/600 [00:14<00:00, 42.34it/s, loss=0.00247]\n",
      "Epoch [36/200] Validation: 100%|██████████| 150/150 [00:01<00:00, 122.94it/s, loss=0.00167]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (0.001745 --> 0.001735). Saving model...\n",
      "Epoch [36/200], Train Loss: 0.0015177057274073983, Val Loss: 0.00173460741682599, LR: 0.0005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [37/200] Training: 100%|██████████| 600/600 [00:13<00:00, 45.92it/s, loss=0.0013] \n",
      "Epoch [37/200] Validation: 100%|██████████| 150/150 [00:01<00:00, 129.92it/s, loss=0.00171]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch [37/200], Train Loss: 0.0015023584155521045, Val Loss: 0.0017418917974767586, LR: 0.0005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [38/200] Training: 100%|██████████| 600/600 [00:13<00:00, 44.47it/s, loss=0.00157]\n",
      "Epoch [38/200] Validation: 100%|██████████| 150/150 [00:01<00:00, 128.46it/s, loss=0.00177]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 2 out of 10\n",
      "Epoch [38/200], Train Loss: 0.0014867247340346996, Val Loss: 0.001755695784619699, LR: 0.0005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [39/200] Training: 100%|██████████| 600/600 [00:13<00:00, 44.47it/s, loss=0.00232]\n",
      "Epoch [39/200] Validation: 100%|██████████| 150/150 [00:01<00:00, 132.97it/s, loss=0.00175]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 3 out of 10\n",
      "Epoch [39/200], Train Loss: 0.0014709281550797945, Val Loss: 0.0017559050877268116, LR: 0.0005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [40/200] Training: 100%|██████████| 600/600 [00:13<00:00, 43.01it/s, loss=0.00234]\n",
      "Epoch [40/200] Validation: 100%|██████████| 150/150 [00:01<00:00, 105.14it/s, loss=0.00181]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 4 out of 10\n",
      "Epoch [40/200], Train Loss: 0.0014597993701075515, Val Loss: 0.0017743292590603232, LR: 0.0005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [41/200] Training: 100%|██████████| 600/600 [00:14<00:00, 41.29it/s, loss=0.00145]\n",
      "Epoch [41/200] Validation: 100%|██████████| 150/150 [00:01<00:00, 108.40it/s, loss=0.00175]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 5 out of 10\n",
      "Epoch [41/200], Train Loss: 0.0014398042069903264, Val Loss: 0.0017832959536463022, LR: 0.0005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [42/200] Training: 100%|██████████| 600/600 [00:13<00:00, 43.78it/s, loss=0.00213]\n",
      "Epoch [42/200] Validation: 100%|██████████| 150/150 [00:01<00:00, 108.18it/s, loss=0.00201]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 6 out of 10\n",
      "Epoch [42/200], Train Loss: 0.001433387571790566, Val Loss: 0.0018310591555200518, LR: 0.00025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [43/200] Training: 100%|██████████| 600/600 [00:14<00:00, 41.12it/s, loss=0.00122] \n",
      "Epoch [43/200] Validation: 100%|██████████| 150/150 [00:01<00:00, 109.19it/s, loss=0.00179]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 7 out of 10\n",
      "Epoch [43/200], Train Loss: 0.0013717608536050344, Val Loss: 0.0017817524028941989, LR: 0.00025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [44/200] Training: 100%|██████████| 600/600 [00:13<00:00, 44.53it/s, loss=0.0022]  \n",
      "Epoch [44/200] Validation: 100%|██████████| 150/150 [00:01<00:00, 114.73it/s, loss=0.00177]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 8 out of 10\n",
      "Epoch [44/200], Train Loss: 0.0013567781621046985, Val Loss: 0.0018124111276119946, LR: 0.00025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [45/200] Training: 100%|██████████| 600/600 [00:13<00:00, 44.72it/s, loss=0.00136]\n",
      "Epoch [45/200] Validation: 100%|██████████| 150/150 [00:01<00:00, 107.49it/s, loss=0.00177]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 9 out of 10\n",
      "Epoch [45/200], Train Loss: 0.0013471641152864322, Val Loss: 0.0018040699209086596, LR: 0.00025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [46/200] Training: 100%|██████████| 600/600 [00:14<00:00, 40.54it/s, loss=0.00168] \n",
      "Epoch [46/200] Validation: 100%|██████████| 150/150 [00:01<00:00, 109.79it/s, loss=0.00173]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 10 out of 10\n",
      "Epoch [46/200], Train Loss: 0.0013312525448660986, Val Loss: 0.001802956215105951, LR: 0.00025\n",
      "Early stopping triggered\n",
      "Training finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    progress_bar_train = tqdm(train_loader, desc=f'Epoch [{epoch+1}/{num_epochs}] Training')\n",
    "    \n",
    "    for inputs, targets in progress_bar_train:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        progress_bar_train.set_postfix({'loss': loss.item()})\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    progress_bar_val = tqdm(val_loader, desc=f'Epoch [{epoch+1}/{num_epochs}] Validation')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in progress_bar_val:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            val_loss += loss.item()\n",
    "            progress_bar_val.set_postfix({'loss': loss.item()})\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "    # Learning rate scheduling\n",
    "    scheduler.step(avg_val_loss)\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "    # Early stopping\n",
    "    early_stopping(avg_val_loss, model)\n",
    "\n",
    "    # Logging\n",
    "    wandb.log({\n",
    "        \"epoch\": epoch + 1,\n",
    "        \"train_loss\": avg_train_loss,\n",
    "        \"val_loss\": avg_val_loss,\n",
    "        \"learning_rate\": current_lr\n",
    "    })\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss}, Val Loss: {avg_val_loss}, LR: {current_lr}')\n",
    "\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping triggered\")\n",
    "        break\n",
    "\n",
    "print(\"Training finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_psnr(target, prediction):\n",
    "    \"\"\"Calculate Peak Signal-to-Noise Ratio\"\"\"\n",
    "    mse = np.mean((target - prediction) ** 2)\n",
    "    if mse == 0:\n",
    "        return float('inf')\n",
    "    max_pixel = np.max(target)\n",
    "    psnr = 20 * np.log10(max_pixel / np.sqrt(mse))\n",
    "    return psnr\n",
    "\n",
    "def calculate_peak_metrics_patch(target, prediction, prominence=0.1):\n",
    "    \"\"\"Calculate peak detection accuracy for full patch predictions\"\"\"\n",
    "    peak_metrics = {'true_peaks': 0, 'predicted_peaks': 0, 'matching_peaks': 0}\n",
    "    \n",
    "    # Iterate over each voxel in the patch\n",
    "    batch_size, num_points, height, width = target.shape\n",
    "    \n",
    "    for b in range(batch_size):\n",
    "        for h in range(height):\n",
    "            for w in range(width):\n",
    "                # Get spectrum for current voxel\n",
    "                true_spectrum = target[b, :, h, w]\n",
    "                pred_spectrum = prediction[b, :, h, w]\n",
    "                \n",
    "                # Find peaks\n",
    "                true_peaks, _ = find_peaks(true_spectrum, prominence=prominence)\n",
    "                pred_peaks, _ = find_peaks(pred_spectrum, prominence=prominence)\n",
    "                \n",
    "                # Count matching peaks\n",
    "                matches = 0\n",
    "                for tp in true_peaks:\n",
    "                    for pp in pred_peaks:\n",
    "                        if abs(tp - pp) <= 1:\n",
    "                            matches += 1\n",
    "                            break\n",
    "                \n",
    "                peak_metrics['true_peaks'] += len(true_peaks)\n",
    "                peak_metrics['predicted_peaks'] += len(pred_peaks)\n",
    "                peak_metrics['matching_peaks'] += matches\n",
    "    \n",
    "    return peak_metrics\n",
    "\n",
    "def measure_inference_time(model, input_tensor, device, num_iterations=100):\n",
    "    \"\"\"Measure average inference time\"\"\"\n",
    "    model.eval()\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_iterations):\n",
    "            _ = model(input_tensor.to(device))\n",
    "    end_time = time.time()\n",
    "    return (end_time - start_time) / num_iterations\n",
    "\n",
    "def count_parameters(model):\n",
    "    \"\"\"Count number of trainable parameters\"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def measure_memory_usage():\n",
    "    \"\"\"Measure current memory usage\"\"\"\n",
    "    process = psutil.Process(os.getpid())\n",
    "    return process.memory_info().rss / 1024 / 1024  # Convert to MB\n",
    "\n",
    "def calculate_spatial_consistency(targets, predictions, window_size=3):\n",
    "    \"\"\"Calculate spatial consistency metric with proper handling of constant patches\"\"\"\n",
    "    batch_size, num_points, height, width = targets.shape\n",
    "    consistency_scores = []\n",
    "    \n",
    "    # Calculate local spatial correlation for each patch\n",
    "    for b in range(min(batch_size, 100)):  # Limit to 100 samples for efficiency\n",
    "        for t in range(num_points):\n",
    "            true_patch = targets[b, t]\n",
    "            pred_patch = predictions[b, t]\n",
    "            \n",
    "            # Skip if either patch is constant\n",
    "            if np.std(true_patch) == 0 or np.std(pred_patch) == 0:\n",
    "                continue\n",
    "                \n",
    "            # Calculate local spatial correlation\n",
    "            try:\n",
    "                correlation = np.corrcoef(true_patch.flatten(), pred_patch.flatten())[0, 1]\n",
    "                if not np.isnan(correlation):\n",
    "                    consistency_scores.append(correlation)\n",
    "            except:\n",
    "                continue\n",
    "    \n",
    "    # Return mean if we have scores, otherwise return 0\n",
    "    return np.mean(consistency_scores) if consistency_scores else 0.0\n",
    "\n",
    "\n",
    "def evaluate_model_metrics(model, val_loader, device):\n",
    "    \"\"\"Evaluate all model metrics for patch-based models\"\"\"\n",
    "    model.eval()\n",
    "    metrics = {}\n",
    "    \n",
    "    # Get sample input for inference time measurement\n",
    "    sample_input, _ = next(iter(val_loader))\n",
    "    metrics['inference_time'] = measure_inference_time(model, sample_input, device)\n",
    "    \n",
    "    all_targets = []\n",
    "    all_predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # Convert to numpy for metric calculation\n",
    "            targets_np = targets.cpu().numpy()\n",
    "            outputs_np = outputs.cpu().numpy()\n",
    "            \n",
    "            all_targets.append(targets_np)\n",
    "            all_predictions.append(outputs_np)\n",
    "    \n",
    "    # Concatenate all batches\n",
    "    all_targets = np.concatenate(all_targets)\n",
    "    all_predictions = np.concatenate(all_predictions)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics['mse'] = np.mean((all_targets - all_predictions) ** 2)\n",
    "    metrics['mae'] = np.mean(np.abs(all_targets - all_predictions))\n",
    "    metrics['psnr'] = calculate_psnr(all_targets, all_predictions)\n",
    "    \n",
    "    # Calculate R² score for each spatial position\n",
    "    r2_scores = []\n",
    "    batch_size, num_points, height, width = all_targets.shape\n",
    "    for h in range(height):\n",
    "        for w in range(width):\n",
    "            true_spectra = all_targets[:, :, h, w].reshape(-1)\n",
    "            pred_spectra = all_predictions[:, :, h, w].reshape(-1)\n",
    "            r2_scores.append(r2_score(true_spectra, pred_spectra))\n",
    "    metrics['r2_score'] = np.mean(r2_scores)\n",
    "    \n",
    "    # Calculate peak metrics for patches\n",
    "    metrics['peak_metrics'] = calculate_peak_metrics_patch(all_targets, all_predictions)\n",
    "    \n",
    "    # System metrics\n",
    "    metrics['memory_usage'] = measure_memory_usage()\n",
    "    metrics['num_parameters'] = count_parameters(model)\n",
    "    \n",
    "    # Add spatial metrics\n",
    "    metrics['spatial_consistency'] = calculate_spatial_consistency(all_targets, all_predictions)\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading best model for final evaluation...\n",
      "Loaded model checkpoint with validation loss: 0.001735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\trpha\\AppData\\Local\\Temp\\ipykernel_29664\\3021863721.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load('best_unet_model_attention_3.pt')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Model Evaluation:\n",
      "MSE: 0.001735\n",
      "MAE: 0.021225\n",
      "PSNR: 27.60 dB\n",
      "R² Score: 0.5772\n",
      "Peak Detection Accuracy: 46.47%\n",
      "Average Inference Time: 5.35 ms\n",
      "Memory Usage: 2794.5 MB\n",
      "Number of Parameters: 1,835,760\n"
     ]
    }
   ],
   "source": [
    "# checkpoint = torch.load('saved_models/best_unet_model_attention_3.pt')\n",
    "# model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "model.eval()\n",
    "final_metrics = evaluate_model_metrics(model, val_loader, device)\n",
    "\n",
    "wandb.log({\n",
    "    \"final_mse\": final_metrics['mse'],\n",
    "    \"final_mae\": final_metrics['mae'],\n",
    "    \"final_psnr\": final_metrics['psnr'],\n",
    "    \"final_r2_score\": final_metrics['r2_score'],\n",
    "    \"peak_detection_accuracy\": final_metrics['peak_metrics']['matching_peaks'] / \n",
    "                             final_metrics['peak_metrics']['true_peaks'],\n",
    "    \"inference_time_ms\": final_metrics['inference_time'] * 1000,\n",
    "    \"memory_usage_mb\": final_metrics['memory_usage'],\n",
    "    \"model_parameters\": final_metrics['num_parameters']\n",
    "})\n",
    "\n",
    "print(\"\\nFinal Model Evaluation:\")\n",
    "print(f\"MSE: {final_metrics['mse']:.6f}\")\n",
    "print(f\"MAE: {final_metrics['mae']:.6f}\")\n",
    "print(f\"PSNR: {final_metrics['psnr']:.2f} dB\")\n",
    "print(f\"R² Score: {final_metrics['r2_score']:.4f}\")\n",
    "print(f\"Peak Detection Accuracy: {final_metrics['peak_metrics']['matching_peaks'] / final_metrics['peak_metrics']['true_peaks']:.2%}\")\n",
    "print(f\"Average Inference Time: {final_metrics['inference_time']*1000:.2f} ms\")\n",
    "print(f\"Memory Usage: {final_metrics['memory_usage']:.1f} MB\")\n",
    "print(f\"Number of Parameters: {final_metrics['num_parameters']:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, targets = next(iter(val_loader))\n",
    "inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(inputs)\n",
    "\n",
    "inputs = inputs.cpu().numpy()\n",
    "targets = targets.cpu().numpy()\n",
    "outputs = outputs.cpu().numpy()\n",
    "\n",
    "example_idx = 0\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(131)\n",
    "plt.imshow(inputs[example_idx, 4, :, :])  # Middle temporal slice (4 out of 8)\n",
    "plt.title('Input 9x9 patch\\n(Middle Time Point)')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.imshow(targets[example_idx, 16, :, :])  # Middle spectral point (16 out of 32)\n",
    "plt.title('Target 9x9 patch\\n(Middle Spectral Point)')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.imshow(outputs[example_idx, 16, :, :])  # Middle spectral point\n",
    "plt.title('Prediction 9x9 patch\\n(Middle Spectral Point)')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot all 81 voxels (9x9) in a single plot\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "# Plot all voxels\n",
    "for i in range(9):\n",
    "    for j in range(9):\n",
    "        plt.plot(targets[example_idx, :, i, j], 'b-', alpha=0.3, label='Target' if i==0 and j==0 else \"\")\n",
    "        plt.plot(outputs[example_idx, :, i, j], 'r--', alpha=0.3, label='Prediction' if i==0 and j==0 else \"\")\n",
    "\n",
    "plt.title('T1 Spectra for All Voxels in 9x9 Patch')\n",
    "plt.xlabel('T1 Index (32 points)')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Print metrics for the whole 9x9 patch\n",
    "patch_mse = np.mean((targets[example_idx] - outputs[example_idx])**2)\n",
    "patch_mae = np.mean(np.abs(targets[example_idx] - outputs[example_idx]))\n",
    "print(f\"Full 9x9 patch MSE: {patch_mse:.6f}\")\n",
    "print(f\"Full 9x9 patch MAE: {patch_mae:.6f}\")\n",
    "\n",
    "# Calculate statistics of individual voxel MSEs\n",
    "voxel_mses = []\n",
    "for i in range(9):\n",
    "    for j in range(9):\n",
    "        mse = np.mean((targets[example_idx, :, i, j] - outputs[example_idx, :, i, j])**2)\n",
    "        voxel_mses.append(mse)\n",
    "\n",
    "print(f\"\\nMean Voxel MSE: {np.mean(voxel_mses):.6f}\")\n",
    "print(f\"Std Voxel MSE: {np.std(voxel_mses):.6f}\")\n",
    "print(f\"Min Voxel MSE: {np.min(voxel_mses):.6f}\")\n",
    "print(f\"Max Voxel MSE: {np.max(voxel_mses):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
